% $Id: problem.tex 1784 2012-04-27 23:29:31Z nicolas.cardozo $
% !TEX root = main.tex

\chapter{Introduction to the Problem}
\label{cha:problem}

\section{Reinforcement Learning}

% Section: Explanation of RL
% common bugs in RL programs.
Before starting with the problem let's understand a little bit how \ac{RL} work and why they are 
important to properly develop in nowdays.
\ac{RL} algorithms were introduced by \citet{Sutton1998}, the idea behind this 
algorithms is to learn by interaction with the environment learning the optimal policy without 
the model of the environment. It uses an agent that learns the value function for a given policy 
through interaction with the environment to predict an optimal solution and based on the value 
function, it continuously develops and learns the optimal policy. One of the most common algorithms
is Q-learning\cite{8836506}, which uses an off-policy control that separates the deferral 
policy from the learning policy and updates the action selection using the Bellman optimal equations
and the e-greed policy. As a normal \ac{RL} algorithms, QLearning involves an agent, a set of states
$S$ and a set of actions $A$ per state. By performing an action $A\in A$, the agent transitions from
one state to another, which will give a $reward$ to the agent. The goal of the agent is to maximize its 
total reward. The following snippet \href{alg:qlearning} of code is the usual pseudo algorithm for 
QLearning off policy algorithms\cite{Sutton1998}.

\begin{algorithm}
\caption{Q Learning Algorithm}\label{alg:qlearning}
\begin{algorithmic}
\Require An environment with states $S$, actions $A$, and reward function $R(s, a)$
\Require A learning rate $\alpha \in [0, 1]$
\Require A discount factor $\gamma \in [0, 1]$
\Require Exploration strategy (e.g., $\epsilon$-greedy)
\State Initialize Q-table $Q(s, a) \gets 0$ for all $s \in S$, $a \in A$
\For{each episode}
    \State Initialize the starting state $s_0$
    \For{each time step in the episode}
        \If{exploration step}
            \State Choose an action $a$ using the exploration strategy (e.g., $\epsilon$-greedy)
        \Else
            \State Choose an action $a$ based on the current Q-values: $a = \arg\max\_a Q(s, a)$
        \EndIf
        \State Take action $a$, observe reward $r$ and next state $s'$
        \State Update Q-value: 
        \[
        Q(s, a) \gets Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a')\right)
        \]
        \State Set $s \gets s'$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\ac{RL} programs have a wide range of applications\cite{8836506}: from control of industial process (improving 
the performance of the on-line learning control system or optimizing temperature control and power consumtion),
to computer networking (improving adaptability of Wireless Sensor Networks to changing situations and eliminating 
the need for system redesing) and even robotics (by providing frameworks and toolkits for designing sophisticated 
behavioral aspects). Additionally, when developing \ac{RL} programs, the usual issues that arise are logic problems, 
regarding the learning rate, wrong deffinition for the rewards, all these problems require stronger frameworks in which 
the development could improve for all \ac{RL} applications, and the first thing to do this by understanding and anlyzing
the current developements.

\section{Problem}

\ac{RL} programs (like all \ac{ML}), 
have a significant limitation and a major problem: they function as a black box. 
This means that it is impossible to know how they reached a decision. This poses 
a problem in many fields, as it often becomes difficult to identify why a model 
fails to converge or why it does not reach an optimal or expected solution as 
anticipated by the developer. In the case of \ac{RL}, this issue is particularly 
pronounced because it is challenging to determine whether the agent is learning 
correctly or if it is learning something inappropriate.

Furthermore, \ac{RL} programs are notoriously difficult to debug due to the extensive 
training periods they require, which demand substantial computational power. This 
complexity makes it incredibly challenging to locate errors, even when using a 
debugger for standard programs. Traditional debugging tools are often inadequate 
for tracing the issues in \ac{RL} due to the intricate and prolonged nature of the 
training processes. This is because \ac{RL} programs have an added difficulty: they 
operate with a different execution cycle compared to traditional programs.

In conventional software, the execution flow is typically linear and predictable, 
allowing developers to trace and debug step-by-step with relative ease. However, 
\ac{RL} programs involve a continuous loop of learning and adaptation, where an agent 
interacts with an environment, receives feedback in the form of rewards or 
penalties, and adjusts its actions accordingly. This cyclical nature makes the 
process much more complex and less transparent.

In \ac{RL}, each decision and its subsequent outcome can affect future decisions, 
creating a dynamic and interdependent series of events. This complexity is 
compounded by the often stochastic nature of environments, where the same action 
can lead to different results in different contexts or iterations. As a result, 
understanding and debugging \ac{RL} programs require not only tracking individual 
decisions but also understanding the long-term effects and patterns that emerge 
over many iterations.

Moreover, the need for substantial computational resources and extended training 
periods adds to the challenge. The iterative process can take hours, days, or 
even weeks, making it impractical to simply restart the training from scratch 
each time an error is encountered. This necessitates advanced tools and methods 
for visualizing the agent's behavior, monitoring its learning progress, and 
pinpointing issues without having to re-run lengthy training sessions.

Therefore, the distinct execution cycle of \ac{RL} programs introduces unique 
challenges that demand specialized debugging and visualization tools to ensure 
effective development and deployment. Addressing these challenges is crucial 
for advancing the field of reinforcement learning and making its applications 
more reliable and interpretable.

Therefore, there is a pressing need for a tool that allows for the visualization 
of an \ac{RL} agent's behavior and facilitates easier debugging of the program. 
Such a tool would significantly enhance the transparency and interpretability 
of \ac{RL} models, making it easier for developers to understand, refine, and correct 
the learning process. This is a gap in the current landscape, and addressing it 
is the focus of this work.


\endinput

